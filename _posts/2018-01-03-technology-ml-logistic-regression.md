---
layout: post
title: "[ML] Logistic Regression"
description: "machine learning"
category: technology
tags: []
modify: 2016-01-03 00:00:00
---
   In the past articles, we've talked about real value prediction model with linear regression. 
When the target prediction is binary classification, logistic regression is coming out.

   Generally, real value is distributed Gaussian probability density. Binary value(data) is distributed 
Bernoulli probability density. Count value is distributed Poisson probability density. So binary
classification is distributed Bernoulli probability density which is in Exponential family. 
   if massage Bernoulli probability density formula {P = p to x * (1−p) to (1−x)} to normal pattern of Exponential family, 
we'll get mean value (expected value)'s probability p which is equal to 1 / (1 + e to -θx). So we
can assume this term as hypothesis.

   H(θ) = 1 / (1 + e to -θx)  
   
   In this formula, 1 / (1 + e to -x) is also called sigmoid function. It can squash input value 
from minus infinity to plus infinity between 0 to 1.

   Now, we get the hypothesis of binary classification (logistic regression). Next, we'll figure 
out cost/loss function of logistic regression.

   As we know logistic regression is in Exponential family. So Max Likelihood Error (MLE) is good 
 fit for loss function cause it's convex. Why we choose MLE? I think there are some reasons as follow.
   1. if sample data is fixed, the probability density of data coming out could be representative
by Likelihood L(θ). Meantime, output value of hypothesis is the expected(mean) value in the 
Bernoulli distribution. They own max probability. So more bigger L(θ) are, more closer the 
probability of output value of hypothesis to expected(mean) value. It means the hypothesis(model) 
trained by fixed sample are more closely/likely/approach to the hypothesis(model) generated by 
expected(mean) value(sample) that it's probability is max in the output distribution(Bernoulli distribution).
   2. Logistic regression is in Exponential family. As Exp-family properties, MLE in Exp-family is 
convex. So we can get global optimal θ by MLE.
   
   So loss function is L(θ) = II p to y * (1−p) to (1−y).

   As so far, we've locked loss function on hypothesis. How can we find the optimal θ through MLE?
   Actually, Gradient descent is better choice. The update rule(normal equation) is as follow:
   
   θ = θ - α * (h(θ) - y) * x
   
   Here, α  is learning rate. 
   Furthermore, we can also solve the optimal θ by Newton method which will use more memory than 
 GSD and it's not fit for big model training in pratice. 
  
 Refer to [CS229 - Logistic Regression](https://www.youtube.com/watch?v=het9HFqo1TQ&t=1245s)
   
   
       
